# -*- coding: utf-8 -*-
"""clinical_baseline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11LLOGuRmAQQNPDeeWAGD3DMGC76GI5Sh
"""

pip install lifelines

pip install scikit-survival

from google.colab import drive
drive.mount('/content/drive', force_remount=True)
import os
import zipfile
import io
import pandas as pd
import shutil
import tempfile
import json
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.feature_selection import RFE
from lifelines import CoxPHFitter
from sksurv.linear_model import CoxPHSurvivalAnalysis
from sksurv.metrics import concordance_index_censored
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from lifelines import KaplanMeierFitter
from sksurv.ensemble import RandomSurvivalForest
from sksurv.util import Surv
from lifelines.statistics import logrank_test

from google.colab import drive
drive.mount('/content/drive')

#loading in the metadata file that contains patient id to file id
clinical_path = "/content/drive/MyDrive/bmi702_final_project/clinical_data/clinical.tsv"

#loading in clinical tsv
clinical_df = pd.read_csv(clinical_path, sep='\t')
clinical_df.head()

columns = clinical_df.columns.values.tolist()
#columns

# finding missing vals
missing_values = clinical_df.isnull().sum()
missing_dash = (clinical_df == "'--").sum()
combined_missing = missing_values + missing_dash
cols_with_missing = combined_missing[combined_missing > 0]
cols_with_missing.head()

# df info
unique_values = clinical_df['case_id'].unique()
print(len(unique_values))

# droping cols with 3000 or more missing vals (basically empty)
cols_to_remove = combined_missing[combined_missing>3000].index
print(cols_to_remove)
clinical_df_cleaned = clinical_df.drop(columns=cols_to_remove)

# aggregating case ids to have 1 row per patient

def aggregate_column(series):
    filtered_series = series[~((series == "'--") | (series.str.lower() == 'not reported'))] # do not include their null char or not reported
    unique_values = filtered_series.unique()

    if len(unique_values) == 1:
        return unique_values[0]  # Return the value if all are the same
    elif len(unique_values) == 0:
      return "'--"
    else:
        return "; ".join(map(str, filtered_series))  # Return a semicolon seperated string if there are multiple values

clinical_df_aggregated = clinical_df_cleaned.groupby('case_id').agg(aggregate_column).reset_index()

# remove rows with missingness above 500
missing_values = clinical_df_aggregated.isnull().sum()
missing_dash = (clinical_df_aggregated == "'--").sum()
combined_missing = missing_values + missing_dash
cols_to_remove = combined_missing[combined_missing>500].index
print(cols_to_remove)
clinical_df_aggregated = clinical_df_aggregated.drop(columns=cols_to_remove)

# remove rows containing "'--" in the target var and time var
clinical_df_aggregated = clinical_df_aggregated[clinical_df_aggregated['vital_status'] != "'--"]
clinical_df_aggregated = clinical_df_aggregated[clinical_df_aggregated['days_to_birth'] != "'--"]

# remove unneccessary cols
cols_to_remove = ['age_is_obfuscated', 'country_of_residence_at_enrollment', 'figo_staging_edition_year', 'clinical_trial_indicator',
                  'course_number', 'number_of_cycles', 'number_of_fractions', 'residual_disease.1', 'treatment_intent_type', 'treatment_outcome',
                  'days_to_treatment_end', 'days_to_treatment_start']
clinical_df_aggregated = clinical_df_aggregated.drop(columns=cols_to_remove)

# replace all '-- with None
clinical_df_aggregated = clinical_df_aggregated.replace("'--", None)

# inferring missing values

# can be = 0
cols_to_0 = ['treatment_dose', 'prescribed_dose']
clinical_df_aggregated[cols_to_0] = clinical_df_aggregated[cols_to_0].fillna(0)

# can be Unknown
cols_to_uk = ['tumor_of_origin', 'residual_disease', 'synchronous_malignancy', 'prior_malignancy', 'method_of_diagnosis', 'race', 'ethnicity', 'year_of_diagnosis']
clinical_df_aggregated[cols_to_uk] = clinical_df_aggregated[cols_to_uk].fillna('Unknown')

# units
cols_to_mg = ['treatment_dose_units', 'prescribed_dose_units']
clinical_df_aggregated[cols_to_mg] = clinical_df_aggregated[cols_to_mg].fillna('mg')

# all cols with ;
cols_with_semicolon = [col for col in clinical_df_aggregated.columns if clinical_df_aggregated[col].astype(str).str.contains(";").any()]
print(cols_with_semicolon)
len(cols_with_semicolon)

# collapsing further from semi-colon sep to a single value

cols_with_multi = ['age_at_diagnosis', 'classification_of_tumor', 'days_to_diagnosis', 'diagnosis_is_primary_disease', 'figo_stage',
                   'morphology', 'primary_diagnosis', 'prior_treatment', 'residual_disease', 'tissue_or_organ_of_origin',
                   'initial_disease_status', 'prescribed_dose', 'prescribed_dose_units', 'therapeutic_agents', 'treatment_dose',
                   'treatment_dose_units', 'treatment_or_therapy', 'treatment_type']

# columns to make binary from presence of yes
col_yes = ['prior_treatment', 'treatment_or_therapy']
for col in col_yes:
  clinical_df_aggregated[col] = clinical_df_aggregated[col].apply(lambda x: 'yes' if 'yes' in x.lower().split('; ') else 'no')

# replace days_to_diagnosis column with highest val
clinical_df_aggregated['days_to_diagnosis'] = clinical_df_aggregated['days_to_diagnosis'].apply(
    lambda value: max(map(int, value.split('; '))) if isinstance(value, str) else value)


# replace certain cols with the latest value
col_latest = ['age_at_diagnosis', 'classification_of_tumor', 'days_to_diagnosis', 'diagnosis_is_primary_disease', 'figo_stage', 'morphology',
              'primary_diagnosis', 'residual_disease', 'initial_disease_status', 'prescribed_dose', 'prescribed_dose_units', 'therapeutic_agents',
              'treatment_dose', 'treatment_dose_units']
for col in col_latest:
  clinical_df_aggregated[col] = clinical_df_aggregated[col].apply(lambda x: str(x).split('; ')[0] if isinstance(x, str) else x)

# treatment type --> add up number of unique treatments patient had
clinical_df_aggregated['treatment_type_count'] = clinical_df_aggregated['treatment_type'].apply(lambda x: len(set(x.split('; '))))

# tissue_or_organ_of_origin -> set of tissues
clinical_df_aggregated['tissue_or_organ_of_origin'] = clinical_df_aggregated['tissue_or_organ_of_origin'].apply(lambda x: '; '.join(sorted(set(x.split('; ')))))
clinical_df_aggregated['tissue_or_organ_of_origin_count'] = clinical_df_aggregated['tissue_or_organ_of_origin'].apply(lambda x: len((set(x.split('; ')))))

# making morphology into 2 cols
clinical_df_aggregated[['histology_code', 'behavior_code']] = clinical_df_aggregated['morphology'].str.split('/', expand=True)

# add time to event col
# days to death for those alive = latest days to diagnosis (highest number in days to diag)
clinical_df_aggregated['time_to_event'] = clinical_df_aggregated['days_to_death'].fillna(clinical_df_aggregated['days_to_diagnosis'])

# cols to drop --> inferred, useless or used
cols_to_drop = ['treatment_type', 'morphology', 'days_to_death', 'days_to_diagnosis',
                'timepoint_category', 'treatment_or_therapy', 'gender', 'tumor_of_origin', 'year_of_diagnosis']
clinical_df_aggregated = clinical_df_aggregated.drop(columns=cols_to_drop)

# final missingness check
clinical_df_aggregated.isna().sum()

clinical_df_aggregated.head(10)

"""Regression Stuff"""

# printing unique values in cols to build encoding dict
for col in clinical_df_aggregated.columns:
  if col in ['case_id', 'case_submitter_id', 'project_id']:
    continue;
  else:
    print(col, list(clinical_df_aggregated[col].unique()))

clinical_df_aggregated.columns

# create encoded df
clinical_df_encoded = clinical_df_aggregated.copy()

# encoding dictionaries for binary and ordinal variables
diagnosis_is_primary_disease = {'true': 1, 'false':0}
prior_treatment = {'yes': 1, 'no':0}
prior_malignancy = {'yes': 1, 'no':0, 'Unknown':0}
synchronous_malignancy = {'Yes': 1, 'No':0, 'Unknown':0}
vital_status = {'Alive': 0, 'Dead': 1}
classification_of_tumor = {'Prior primary':0, 'primary':1, 'Subsequent Primary':2, 'Synchronous primary':3, 'recurrence':4, 'metastasis':5}
figo_stage = {'Stage IA':0,
    'Stage IB1':0,
    'Stage IB':0,
    'Stage IC':0,
    'Stage I':0,
    'Stage IIA':1,
    'Stage IIB':1,
    'Stage II':1,
    'Stage IIIA':2,
    'Stage IIIB':2,
    'Stage IIIC1':2,
    'Stage IIIC2':2,
    'Stage IIIC':2,
    'Stage III':2,
    'Stage IVA':3,
    'Stage IVB':3,
    'Stage IV':3}
site_of_resection_or_biopsy = {'Endometrium':0, 'Fundus uteri':1}
tumor_grade = {'G1':0, 'G2':1, 'G3':2, 'High Grade':3}
residual_disease = {'Unknown':0, 'R0':0, 'R1':1, 'R2':2, 'RX':3}
initial_disease_status = {'Initial Diagnosis':0, 'Progressive Disease':1, 'Recurrent Disease':2, 'Persistent Disease':3}

# mapping encoding to value
clinical_df_encoded['diagnosis_is_primary_disease'] = clinical_df_encoded['diagnosis_is_primary_disease'].map(diagnosis_is_primary_disease)
clinical_df_encoded['prior_treatment'] = clinical_df_encoded['prior_treatment'].map(prior_treatment)
clinical_df_encoded['prior_malignancy'] = clinical_df_encoded['prior_malignancy'].map(prior_malignancy)
clinical_df_encoded['synchronous_malignancy'] = clinical_df_encoded['synchronous_malignancy'].map(synchronous_malignancy)
clinical_df_encoded['vital_status'] = clinical_df_encoded['vital_status'].map(vital_status)
clinical_df_encoded['classification_of_tumor'] = clinical_df_encoded['classification_of_tumor'].map(classification_of_tumor)
clinical_df_encoded['figo_stage'] = clinical_df_encoded['figo_stage'].map(figo_stage)
clinical_df_encoded['site_of_resection_or_biopsy'] = clinical_df_encoded['site_of_resection_or_biopsy'].map(site_of_resection_or_biopsy)
clinical_df_encoded['tumor_grade'] = clinical_df_encoded['tumor_grade'].map(tumor_grade)
clinical_df_encoded['residual_disease'] = clinical_df_encoded['residual_disease'].map(residual_disease)
clinical_df_encoded['initial_disease_status'] = clinical_df_encoded['initial_disease_status'].map(initial_disease_status)

#numeric cols
cols_numeric = ['age_at_index', 'days_to_birth', 'age_at_diagnosis',
                'time_to_event', 'histology_code', 'behavior_code',
                'treatment_type_count', 'tissue_or_organ_of_origin_count',
                'treatment_dose']

for col in cols_numeric:
    clinical_df_encoded[col] = pd.to_numeric(clinical_df_encoded[col], errors='coerce')

# prescribed_dose : str -> flt -> int
clinical_df_encoded['prescribed_dose'] = clinical_df_encoded['prescribed_dose'].astype(float)
clinical_df_encoded['prescribed_dose'] = clinical_df_encoded['prescribed_dose'].astype(int)

# one-hot endocing of remaning cols
cols_one_hot = ['icd_10_code', 'method_of_diagnosis', 'primary_diagnosis', 'prescribed_dose_units',
                'therapeutic_agents', 'treatment_dose_units', 'ethnicity', 'race']
clinical_df_encoded = pd.get_dummies(clinical_df_encoded, columns=cols_one_hot, drop_first=True, dtype=int)

# tissue_or_organ_of_origin one-hot encoding
tissues = clinical_df_encoded['tissue_or_organ_of_origin'].str.get_dummies(sep='; ')
tissues.columns = [f'tissue_or_organ_of_origin {col}' for col in tissues.columns]
clinical_df_encoded = pd.concat([clinical_df_encoded, tissues.iloc[:, 1:]], axis=1)
clinical_df_encoded = clinical_df_encoded.drop(columns=['tissue_or_organ_of_origin'])

# checking that all cols are encoded and numeric
non_numeric_cols = clinical_df_encoded.select_dtypes(exclude=['int64']).columns
non_numeric_cols

# add +1 to time_to_event to avoid 0
clinical_df_encoded['time_to_event'] = clinical_df_encoded['time_to_event'] + 1

# correlation

df = clinical_df_encoded.iloc[:, 3:]
correlation_matrix = df.corr()

# find highly correlaed cols
threshold = 0.8
mask = np.triu(np.ones(correlation_matrix.shape), k=1)
high_corr = correlation_matrix.where(mask > 0).stack().reset_index()
high_corr.columns = ['Variable_1', 'Variable_2', 'Correlation']
high_corr_filtered = high_corr[high_corr['Correlation'].abs() > threshold]
high_corr_filtered

# columns to remove (correlation high > 0.8)
cols_to_remove = ['days_to_birth', 'age_at_diagnosis', 'diagnosis_is_primary_disease', 'primary_diagnosis_Pheochromocytoma, NOS',
                  'tissue_or_organ_of_origin Endometrium', 'tissue_or_organ_of_origin Fundus uteri', 'tissue_or_organ_of_origin Corpus uteri',
                  'tissue_or_organ_of_origin Kidney, NOS', 'primary_diagnosis_Serous cystadenocarcinoma, NOS', 'tissue_or_organ_of_origin Blood',
                  'tissue_or_organ_of_origin Cecum', 'therapeutic_agents_Cyclophosphamide', 'treatment_dose_units_cGy']

clinical_df_encoded = clinical_df_encoded.drop(columns=cols_to_remove)

# remove features with very low variance < 0.05
df = clinical_df_encoded.drop(columns=['case_id', 'project_id', 'case_submitter_id', 'vital_status', 'time_to_event'])
feature_variance = df.var()
threshold = 0.05
drop_low_var = feature_variance[feature_variance < threshold].index
clinical_df_encoded = clinical_df_encoded.drop(columns=drop_low_var)

# how many features remain
clinical_df_encoded.shape

clinical_df_encoded.to_csv('/content/drive/MyDrive/bmi702_final_project/clinical_encoded_not_scaled.tsv', sep='\t', index=False)

# scaling of ordinal and continuous variables
columns_no_scale = ['case_id', 'project_id', 'case_submitter_id', 'vital_status', 'time_to_event']
continuous_cols = ['age_at_index', 'prescribed_dose', 'treatment_dose']
discrete_cold = ['tissue_or_organ_of_origin_count', 'treatment_type_count']
ordinal_cols = ['classification_of_tumor', 'figo_stage', 'residual_disease', 'tumor_grade', 'initial_disease_status']
binary_cols = ['prior_malignancy', 'prior_treatment', 'histology_code',
       'method_of_diagnosis_Dilation and Curettage Procedure',
       'method_of_diagnosis_Surgical Resection',
       'primary_diagnosis_Endometrioid adenocarcinoma, NOS',
       'prescribed_dose_units_mg', 'therapeutic_agents_Carboplatin',
       'therapeutic_agents_Paclitaxel', 'therapeutic_agents_Tamoxifen',
       'treatment_dose_units_mg', 'ethnicity_not hispanic or latino',
       'race_black or african american', 'race_white']

# z-scale only continuous
df = clinical_df_encoded.drop(columns=['case_id', 'project_id', 'case_submitter_id'])
scaler = StandardScaler()
df[continuous_cols] = scaler.fit_transform(df[continuous_cols])

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

# linearity check
cph = CoxPHFitter()
cph.fit(df, duration_col='time_to_event', event_col='vital_status')
resid = cph.compute_residuals(df, 'martingale')
resid.head()

linear_check = continuous_cols

for feature in linear_check:
  plt.figure(figsize=(8, 4))
  sns.scatterplot(
      x=df[feature],
      y=resid['martingale'],
      hue=df['vital_status'],  # color by event status
      palette={0: '#008fd5', 1: '#fc4f30'},
      alpha=0.75
  )
  plt.title(f'Martingale Residuals vs {feature}')
  plt.xlabel(feature)
  plt.ylabel('Martingale Residuals')
  plt.legend(title='Vital Status', loc='best')
  plt.show()

# Cox Regression

#feature selection

# get features, target
x = df.drop(columns=['vital_status', 'time_to_event'])
y = df['vital_status']

# select 10 features
model = LogisticRegression(max_iter=10000)
selector = RFE(model, n_features_to_select=10)
x_rfe = selector.fit_transform(x, y)

# print selected features
selected_features = x.columns[selector.support_]
print("Selected Features after RFE:", selected_features)


df_selected = df[selected_features].copy()

df_selected['time_to_event'] = df['time_to_event']
df_selected['vital_status'] = df['vital_status']

# fit cox proportional hazards model on selected features
cph = CoxPHFitter()
cph.fit(df_selected, duration_col='time_to_event', event_col='vital_status')

# summary of cox model
print(cph.summary)

# plotting log hazard raios
cph.plot()
plt.title('Cox Proportional Hazards Model - Hazard Ratios')
plt.show()

# kaplan meir curve for figo_stage

figo_stage = {v: k for k, v in figo_stage.items()}


for grp in df_selected['figo_stage'].unique():
    subset = df_selected[df_selected['figo_stage'] == grp]
    kmf = KaplanMeierFitter()
    label = figo_stage.get(grp, f'Group {grp}')
    kmf.fit(subset['time_to_event'], subset['vital_status'], label=label)
    kmf.plot_survival_function()

plt.title(f'Kaplan-Meier Survival Curve for UCEC Cancer Stage')
plt.xlabel('Time to Event')
plt.ylabel('Survival Probability')

plt.show()

# kaplan meir curve for prior_treatment
prior_treatment = {v: k for k, v in prior_treatment.items()}

for grp in df_selected['prior_treatment'].unique():
    subset = df_selected[df_selected['prior_treatment'] == grp]
    kmf = KaplanMeierFitter()
    label = prior_treatment.get(grp, f'Group {grp}')
    kmf.fit(subset['time_to_event'], subset['vital_status'], label=label)
    kmf.plot_survival_function()

plt.title(f'Kaplan-Meier Survival Curve for Treatment Before Sample Collection')
plt.xlabel('Time to Event')
plt.ylabel('Survival Probability')

plt.show()

# kaplan meir curve for prior_malignancy

prior_malignancy = {0:'no', 1:'yes'}

for grp in df_selected['prior_malignancy'].unique():
    subset = df_selected[df_selected['prior_malignancy'] == grp]
    kmf = KaplanMeierFitter()
    label = prior_malignancy.get(grp, f'Group {grp}')
    kmf.fit(subset['time_to_event'], subset['vital_status'], label=label)
    kmf.plot_survival_function()

plt.title(f'Kaplan-Meier Survival Curve for Prior Malignancy')
plt.xlabel('Time to Event')
plt.ylabel('Survival Probability')

plt.show()

# Random survival forest

# scaling
df = clinical_df_encoded.drop(columns=['case_id', 'project_id', 'case_submitter_id'])
scaler = StandardScaler()
df[continuous_cols] = scaler.fit_transform(df[continuous_cols])

# feature selection

# get features, target
a = df.drop(columns=['vital_status', 'time_to_event'])
b = df['vital_status']

# data format
b_structured = Surv.from_dataframe('vital_status', 'time_to_event', df)

# RSF
rsf = RandomSurvivalForest(n_estimators=200,
                           min_samples_split=5,
                           min_samples_leaf=5,
                           max_features="sqrt",
                           n_jobs=-1,
                           random_state=702)
rsf.fit(a, b_structured)

# model performance
from sksurv.metrics import concordance_index_censored

# C-index
c_index = concordance_index_censored(b_structured['vital_status'], b_structured['time_to_event'], rsf.predict(a))[0]
print(f"C-index: {c_index:.3f}")

from sklearn.inspection import permutation_importance
# feature importance

# permutation importance
result = permutation_importance(
    rsf, a, b_structured,
    n_repeats=10,
    random_state=702,
    n_jobs=-1
)

# Sort features by importance
importances = result.importances_mean

# Put into DataFrame
feature_importance_df = pd.DataFrame({
    "Feature": a.columns,
    "Importance": importances
}).sort_values(by="Importance", ascending=False)

print(feature_importance_df)

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

# plot feature importances
plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])
plt.xlabel('Feature Importance')
plt.ylabel('Features')
plt.title('Feature Importance from Random Survival Forest')
plt.show()

# Commented out IPython magic to ensure Python compatibility.
# Normalized importance

importances_normalized = importances / importances.sum()

import matplotlib.pyplot as plt
# %matplotlib inline


# Create a DataFrame for normalized feature importances
feature_importance_norm__df = pd.DataFrame({
    "Feature": a.columns,
    "Importance_Normalized": importances_normalized
}).sort_values(by="Importance_Normalized", ascending=False)

# Plot the normalized feature importance
plt.figure(figsize=(10, 6))
plt.barh(feature_importance_norm__df['Feature'], feature_importance_norm__df['Importance_Normalized'])
plt.xlabel('Normalized Feature Importance')
plt.ylabel('Features')
plt.title('Normalized Feature Importance from Permutation Importance')
plt.show()

# risk scores
risk_scores = rsf.predict(a)

risk_df = pd.DataFrame({
    "Patient_ID": clinical_df_encoded['case_id'],
    "Status": df['vital_status'],
    "Time_to_Event": df['time_to_event'],
    "Risk_Score": risk_scores,
})

print(risk_df.head())

# Commented out IPython magic to ensure Python compatibility.

import matplotlib.pyplot as plt
# %matplotlib inline

plt.figure(figsize=(8, 6))
sns.boxplot(x='Status', y='Risk_Score', data=risk_df)
plt.xticks([0, 1], ['Alive', 'Dead'])
plt.xlabel('Vital Status')
plt.ylabel('Risk Score')
plt.title('Risk Score Distribution by Vital Status')
plt.show()

# Commented out IPython magic to ensure Python compatibility.

median_risk = risk_df['Risk_Score'].median()
risk_df['Risk_Group'] = np.where(risk_df['Risk_Score'] <= median_risk, 'Low Risk', 'High Risk')

kmf_low = KaplanMeierFitter()
kmf_high = KaplanMeierFitter()

low_risk = risk_df[risk_df['Risk_Group'] == 'Low Risk']
high_risk = risk_df[risk_df['Risk_Group'] == 'High Risk']

kmf_low.fit(durations=low_risk['Time_to_Event'], event_observed=low_risk['Status'], label='Low Risk')
kmf_high.fit(durations=high_risk['Time_to_Event'], event_observed=high_risk['Status'], label='High Risk')


import matplotlib.pyplot as plt
# %matplotlib inline

plt.figure(figsize=(10, 6))
ax = plt.subplot(111)
kmf_low.plot_survival_function(ax=ax)
kmf_high.plot_survival_function(ax=ax)
plt.title('Kaplan-Meier Survival Curves by Risk Group')
plt.xlabel('Time to Event')
plt.ylabel('Survival Probability')
plt.grid(True)
plt.show()

results = logrank_test(
    low_risk['Time_to_Event'], high_risk['Time_to_Event'],
    event_observed_A=low_risk['Status'],
    event_observed_B=high_risk['Status']
)

print(f"Log-rank test p-value: {results.p_value}")



# Commented out IPython magic to ensure Python compatibility.
# survival function curves

import matplotlib.pyplot as plt
# %matplotlib inline

patient_index = 300
surv_func = rsf.predict_survival_function(a.iloc[patient_index:patient_index+1])[0]

plt.plot(surv_func.x, surv_func.y, label=f"Patient {patient_index}")
plt.xlabel("Time")
plt.ylabel("Survival Probability")
plt.title(f"Survival Curve for Patient {patient_index}")
plt.legend()
plt.grid(True)
plt.show()

import os
import zipfile
import io
import pandas as pd
import shutil
import tempfile
import json
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.feature_selection import RFE
from lifelines import CoxPHFitter
from sksurv.linear_model import CoxPHSurvivalAnalysis
from sksurv.metrics import concordance_index_censored
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from lifelines import KaplanMeierFitter
from sksurv.ensemble import RandomSurvivalForest
from sksurv.util import Surv
from lifelines.statistics import logrank_test
import pandas as pd
import numpy as np
from lifelines.utils import k_fold_cross_validation
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.inspection import permutation_importance

#  More RSF tuning (n_estimators, min_samples_split, etc)
X = df.drop(columns=["vital_status", "time_to_event"])  # features only
y_survival = np.array([(bool(e), t) for e, t in zip(df["vital_status"], df["time_to_event"])],
                      dtype=[('event', bool), ('time', float)])

y_classification = df["vital_status"]

# Define different RSF configurations to try
rsf_settings = [
    {"n_estimators": 100, "min_samples_split": 10, "min_samples_leaf": 15},
    {"n_estimators": 300, "min_samples_split": 5, "min_samples_leaf": 5},
    {"n_estimators": 500, "min_samples_split": 2, "min_samples_leaf": 1},
]

# Store results
tuning_results = []

# Cross-validation setup
kf = KFold(n_splits=10, shuffle=True, random_state=702)

# Try each RSF configuration
for settings in rsf_settings:
    cindex_scores = []
    for train_idx, test_idx in kf.split(X):
        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
        y_train, y_test = y_survival[train_idx], y_survival[test_idx]

        rsf = RandomSurvivalForest(
            n_estimators=settings["n_estimators"],
            min_samples_split=settings["min_samples_split"],
            min_samples_leaf=settings["min_samples_leaf"],
            random_state=702,
            n_jobs=-1
        )
        rsf.fit(X_train, y_train)
        risk_scores = rsf.predict(X_test)

        cindex_result = concordance_index_censored(
            y_test["event"],
            y_test["time"],
            risk_scores
        )
        cindex = cindex_result[0]
        cindex_scores.append(cindex)

    avg_cindex = np.mean(cindex_scores)
    tuning_results.append({
        "n_estimators": settings["n_estimators"],
        "min_samples_split": settings["min_samples_split"],
        "min_samples_leaf": settings["min_samples_leaf"],
        "Average C-index": avg_cindex
    })

# Save tuning results
tuning_df = pd.DataFrame(tuning_results)
tuning_df.to_csv('/content/drive/My Drive/rsf_tuning_results_clinical_only.csv', index=False)

print("Random Survival Forest tuning results:")
print(tuning_df)

# choosing best rsf for final model on all features (not too overfit)

# get features, target
a = df.drop(columns=['vital_status', 'time_to_event'])
b = df['vital_status']

# data format
b_structured = Surv.from_dataframe('vital_status', 'time_to_event', df)

rsf = RandomSurvivalForest(
            n_estimators=100,
            min_samples_split=10,
            min_samples_leaf=15,
            random_state=702,
            n_jobs=-1
        )
rsf.fit(a, b_structured)

# C-index
c_index = concordance_index_censored(b_structured['vital_status'], b_structured['time_to_event'], rsf.predict(a))[0]
print(f"C-index: {c_index:.3f}")

# feature importance

# permutation importance
result = permutation_importance(
    rsf, a, b_structured,
    n_repeats=10,
    random_state=702,
    n_jobs=-1
)

# Sort features by importance
importances = result.importances_mean

# Put into DataFrame
feature_importance_df = pd.DataFrame({
    "Feature": a.columns,
    "Importance": importances
}).sort_values(by="Importance", ascending=False)

print(feature_importance_df)

# Commented out IPython magic to ensure Python compatibility.
# Normalized importance

importances_normalized = importances / importances.sum()

import matplotlib.pyplot as plt
# %matplotlib inline


# Create a DataFrame for normalized feature importances
feature_importance_norm__df = pd.DataFrame({
    "Feature": a.columns,
    "Importance_Normalized": importances_normalized
}).sort_values(by="Importance_Normalized", ascending=False)

# Plot the normalized feature importance
plt.figure(figsize=(5, 10))
plt.barh(feature_importance_norm__df['Feature'], feature_importance_norm__df['Importance_Normalized'])
plt.xlabel('Normalized Feature Importance')
plt.ylabel('Features')
plt.title('Normalized Feature Importance from Permutation Importance')
plt.show()

feature_importance_df.sort_values(by='Importance', ascending=False)

# Commented out IPython magic to ensure Python compatibility.

median_risk = risk_df['Risk_Score'].median()
risk_df['Risk_Group'] = np.where(risk_df['Risk_Score'] <= median_risk, 'Low Risk', 'High Risk')

kmf_low = KaplanMeierFitter()
kmf_high = KaplanMeierFitter()

low_risk = risk_df[risk_df['Risk_Group'] == 'Low Risk']
high_risk = risk_df[risk_df['Risk_Group'] == 'High Risk']

kmf_low.fit(durations=low_risk['Time_to_Event'], event_observed=low_risk['Status'], label='Low Risk')
kmf_high.fit(durations=high_risk['Time_to_Event'], event_observed=high_risk['Status'], label='High Risk')


import matplotlib.pyplot as plt
# %matplotlib inline

plt.figure(figsize=(10, 6))
ax = plt.subplot(111)
kmf_low.plot_survival_function(ax=ax)
kmf_high.plot_survival_function(ax=ax)
plt.title('Kaplan-Meier Survival Curves by Risk Group (RSF Model)')
plt.xlabel('Time to Event')
plt.ylabel('Survival Probability')
plt.grid(True)
plt.show()

results = logrank_test(
    low_risk['Time_to_Event'], high_risk['Time_to_Event'],
    event_observed_A=low_risk['Status'],
    event_observed_B=high_risk['Status']
)

print(f"Log-rank test p-value: {results.p_value}")

from sklearn.feature_selection import RFE
from lifelines import CoxPHFitter
from sksurv.linear_model import CoxPHSurvivalAnalysis
from sksurv.metrics import concordance_index_censored
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from lifelines import KaplanMeierFitter
from sksurv.ensemble import RandomSurvivalForest
from sksurv.util import Surv
from lifelines.statistics import logrank_test
import pandas as pd
import numpy as np
from lifelines.utils import k_fold_cross_validation
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.inspection import permutation_importance

X = df.drop(columns=["vital_status", "time_to_event"])  # features only
y_survival = np.array([(bool(e), t) for e, t in zip(df["vital_status"], df["time_to_event"])],
                      dtype=[('event', bool), ('time', float)])

y_classification = df["vital_status"]

# Combine X and y for Cox
cox_data = X.copy()
cox_data["time_to_event"] = df["time_to_event"]
cox_data["vital_status"] = df["vital_status"]
cox_data = cox_data.rename(columns={"time_to_event": "T", "vital_status": "E"})


# Initialize and train the Cox model with regularization
cox_model = CoxPHFitter(penalizer=0.1)  # <-- Added penalizer
cox_model.fit(cox_data, duration_col="T", event_col="E")

# Print summary
cox_model.print_summary()

# Perform 10-fold cross-validation for Cox
kf = KFold(n_splits=10, shuffle=True, random_state=702)

cox_cv_scores = k_fold_cross_validation(
    cox_model,
    cox_data,
    duration_col="T",
    event_col="E",
    k=10,
    scoring_method="concordance_index"
)

print("\nCox Model 10-Fold Cross-Validation C-Index:", round(np.mean(cox_cv_scores), 4))

# Predict risk scores (higher = higher hazard = higher risk)
cox_risk_scores = cox_model.predict_partial_hazard(cox_data)

# make a DataFrame for risk, event, time
cox_df_risk = pd.DataFrame({
    "risk_score": cox_risk_scores.values.flatten(),  # flatten because it's a Series
    "event": cox_data["E"],
    "time": cox_data["T"]
})

# Split into Low-risk and High-risk groups
# Median split
cox_median_risk = cox_df_risk["risk_score"].median()
cox_df_risk["risk_group"] = ["High" if x > cox_median_risk else "Low" for x in cox_df_risk["risk_score"]]

print(cox_df_risk["risk_group"].value_counts())

# Run Log-Rank Test for Cox risk groups

from lifelines.statistics import logrank_test

# Separate data
cox_low_risk = cox_df_risk[cox_df_risk["risk_group"] == "Low"]
cox_high_risk = cox_df_risk[cox_df_risk["risk_group"] == "High"]

# Log-rank test
cox_logrank_results = logrank_test(
    cox_low_risk["time"], cox_high_risk["time"],
    event_observed_A=cox_low_risk["event"],
    event_observed_B=cox_high_risk["event"]
)

print("Cox Model Log-Rank Test p-value:", cox_logrank_results.p_value)